{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdNTJSJHJaJMpcw3UycXUU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tnotstar/machine-learning-zoomcamp/blob/master/cohorts/2025/08-deep-learning/submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Zq_P_owVV8OX"
      },
      "outputs": [],
      "source": [
        "# Cell 0: Library imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Reproducibility seed\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "dkb8uq3OZD0I"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Download and Unzip Data\n",
        "!wget -q -NL https://github.com/SVizor42/ML_Zoomcamp/releases/download/straight-curly-data/data.zip\n",
        "!unzip -q -o -d . data.zip"
      ],
      "metadata": {
        "id": "kczVJ5_1wubT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Model Architecture\n",
        "class HairClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HairClassifier, self).__init__()\n",
        "        # Input shape: (3, 200, 200)\n",
        "\n",
        "        # 1. Convolutional Layer\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # 2. Max Pooling\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        # 3. Flatten\n",
        "        # We need to calculate the flattened size.\n",
        "        # Input (200, 200) -> Conv (3x3) -> (198, 198) -> Pool (2x2) -> (99, 99)\n",
        "        # 32 filters * 99 * 99\n",
        "        self.flatten_size = 32 * 99 * 99\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # 4. Linear Layer 1\n",
        "        self.fc1 = nn.Linear(self.flatten_size, 64)\n",
        "\n",
        "        # 5. Output Layer\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize Model\n",
        "model = HairClassifier().to(device)\n",
        "\n",
        "# Q1: Loss Function\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.002, momentum=0.8)"
      ],
      "metadata": {
        "id": "RhfGEHFnuk8J"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Question 2 - Count Parameters\n",
        "# We can use the manual counting method provided in the prompt\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Question 2 - Total parameters: {total_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUyj5UJmuxMQ",
        "outputId": "39f74d90-ea92-44ea-e352-25512d159ca5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 2 - Total parameters: 20073473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Data Preparation (Phase 1)\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((200, 200)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "# Assuming the zip created a 'data' folder with 'train' and 'test' subfolders\n",
        "train_dir = './data/train'\n",
        "test_dir = './data/test'\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transforms)\n",
        "validation_dataset = datasets.ImageFolder(root=test_dir, transform=train_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True, num_workers=2)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=20, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "gABiyY_Avgy6"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Training Loop (Phase 1)\n",
        "num_epochs = 10\n",
        "history = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        labels = labels.float().unsqueeze(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    epoch_acc = correct_train / total_train\n",
        "    history['loss'].append(epoch_loss)\n",
        "    history['acc'].append(epoch_acc)\n",
        "\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in validation_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            labels = labels.float().unsqueeze(1)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_running_loss += loss.item() * images.size(0)\n",
        "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_epoch_loss = val_running_loss / len(validation_dataset)\n",
        "    val_epoch_acc = correct_val / total_val\n",
        "    history['val_loss'].append(val_epoch_loss)\n",
        "    history['val_acc'].append(val_epoch_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
        "          f\"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, \"\n",
        "          f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_cJfYmCvmgW",
        "outputId": "4e159811-4990-45b0-8113-79961038826a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1/10, Loss: 0.6654, Acc: 0.6262, Val Loss: 0.6102, Val Acc: 0.6617\n",
            "Epoch 2/10, Loss: 0.5479, Acc: 0.7087, Val Loss: 0.6303, Val Acc: 0.6468\n",
            "Epoch 3/10, Loss: 0.4858, Acc: 0.7638, Val Loss: 0.6991, Val Acc: 0.5970\n",
            "Epoch 4/10, Loss: 0.4806, Acc: 0.7650, Val Loss: 0.6097, Val Acc: 0.7015\n",
            "Epoch 5/10, Loss: 0.4403, Acc: 0.7863, Val Loss: 0.6215, Val Acc: 0.6517\n",
            "Epoch 6/10, Loss: 0.3493, Acc: 0.8500, Val Loss: 0.6453, Val Acc: 0.6716\n",
            "Epoch 7/10, Loss: 0.3005, Acc: 0.8775, Val Loss: 0.6953, Val Acc: 0.6816\n",
            "Epoch 8/10, Loss: 0.2746, Acc: 0.8938, Val Loss: 0.6370, Val Acc: 0.7214\n",
            "Epoch 9/10, Loss: 0.2119, Acc: 0.9225, Val Loss: 0.6828, Val Acc: 0.7114\n",
            "Epoch 10/10, Loss: 0.1419, Acc: 0.9525, Val Loss: 0.8263, Val Acc: 0.6965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Answer Q3 and Q4\n",
        "import numpy as np\n",
        "\n",
        "# Question 3: Median of training accuracy\n",
        "median_train_acc = np.median(history['acc'])\n",
        "print(f\"Question 3 - Median Training Acc: {median_train_acc:.2f}\")\n",
        "\n",
        "# Question 4: Standard Deviation of training loss\n",
        "std_train_loss = np.std(history['loss'])\n",
        "print(f\"Question 4 - Std Training Loss: {std_train_loss:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgevaJY-v4OJ",
        "outputId": "df46387c-57e1-4eb9-d5a1-f54a846a0d16"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 3 - Median Training Acc: 0.82\n",
            "Question 4 - Std Training Loss: 0.154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Data Augmentation Setup\n",
        "train_aug_transforms = transforms.Compose([\n",
        "    transforms.RandomRotation(50),\n",
        "    transforms.RandomResizedCrop(200, scale=(0.9, 1.0), ratio=(0.9, 1.1)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "# Reload the training dataset with the new transforms\n",
        "# Note: We do NOT re-initialize the model or optimizer\n",
        "train_dataset_aug = datasets.ImageFolder(root=train_dir, transform=train_aug_transforms)\n",
        "train_loader_aug = DataLoader(train_dataset_aug, batch_size=20, shuffle=True, num_workers=2)\n",
        "\n",
        "# Create a new history dict for the continuation\n",
        "history_aug = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n",
        "\n",
        "print(\"Continuing training with augmentation...\")\n",
        "# Run for another 10 epochs\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for images, labels in train_loader_aug: # Use the augmented loader\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        labels = labels.float().unsqueeze(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset_aug)\n",
        "    epoch_acc = correct_train / total_train\n",
        "    history_aug['loss'].append(epoch_loss)\n",
        "    history_aug['acc'].append(epoch_acc)\n",
        "\n",
        "    # Validation (Loader remains the same as before, no augmentation on test set)\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in validation_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            labels = labels.float().unsqueeze(1)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_running_loss += loss.item() * images.size(0)\n",
        "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_epoch_loss = val_running_loss / len(validation_dataset)\n",
        "    val_epoch_acc = correct_val / total_val\n",
        "    history_aug['val_loss'].append(val_epoch_loss)\n",
        "    history_aug['val_acc'].append(val_epoch_acc)\n",
        "\n",
        "    print(f\"Aug Epoch {epoch+1}/10, \"\n",
        "          f\"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, \"\n",
        "          f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dE_gfdTwx3TJ",
        "outputId": "5264e0fb-aa34-4af6-f334-b0f1835dc407"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Continuing training with augmentation...\n",
            "Aug Epoch 1/10, Loss: 0.6685, Acc: 0.6525, Val Loss: 0.6630, Val Acc: 0.6816\n",
            "Aug Epoch 2/10, Loss: 0.5434, Acc: 0.7125, Val Loss: 0.6875, Val Acc: 0.6269\n",
            "Aug Epoch 3/10, Loss: 0.5330, Acc: 0.7188, Val Loss: 0.5545, Val Acc: 0.7015\n",
            "Aug Epoch 4/10, Loss: 0.5334, Acc: 0.7212, Val Loss: 0.5752, Val Acc: 0.7164\n",
            "Aug Epoch 5/10, Loss: 0.4792, Acc: 0.7638, Val Loss: 0.5288, Val Acc: 0.7313\n",
            "Aug Epoch 6/10, Loss: 0.4904, Acc: 0.7562, Val Loss: 0.4977, Val Acc: 0.7463\n",
            "Aug Epoch 7/10, Loss: 0.4871, Acc: 0.7512, Val Loss: 0.4742, Val Acc: 0.7612\n",
            "Aug Epoch 8/10, Loss: 0.4882, Acc: 0.7562, Val Loss: 0.4657, Val Acc: 0.7612\n",
            "Aug Epoch 9/10, Loss: 0.4412, Acc: 0.7913, Val Loss: 0.6119, Val Acc: 0.6517\n",
            "Aug Epoch 10/10, Loss: 0.4558, Acc: 0.7788, Val Loss: 0.5128, Val Acc: 0.7512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Answer Q5 and Q6\n",
        "\n",
        "# Question 5: Mean of test (validation) loss for all epochs trained with augmentations\n",
        "mean_val_loss_aug = np.mean(history_aug['val_loss'])\n",
        "print(f\"Question 5 - Mean Test Loss (Augmented): {mean_val_loss_aug:.3f}\")\n",
        "\n",
        "# Question 6: Average of test (validation) accuracy for the last 5 epochs (6 to 10)\n",
        "# Python list indexing: the last 5 items are [-5:]\n",
        "avg_val_acc_last_5 = np.mean(history_aug['val_acc'][-5:])\n",
        "print(f\"Question 6 - Avg Test Acc (Last 5 epochs): {avg_val_acc_last_5:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZcWsMySyrZp",
        "outputId": "cbfcda61-95d1-4833-fe2b-ecfd2be5cd84"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 5 - Mean Test Loss (Augmented): 0.557\n",
            "Question 6 - Avg Test Acc (Last 5 epochs): 0.73\n"
          ]
        }
      ]
    }
  ]
}