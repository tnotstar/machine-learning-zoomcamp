{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJIYa/CBXwBD85FCJqD3jK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tnotstar/machine-learning-zoomcamp/blob/master/cohorts/2025/08-deep-learning/submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning Zoomcamp (Cohort 2025) #"
      ],
      "metadata": {
        "id": "Hpe8qijY3RL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 08. Neural Networks and Deep Learning ##"
      ],
      "metadata": {
        "id": "MzCOh7a_3Xss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Homework (Submitted at 2025-12-01)** ###\n",
        "\n"
      ],
      "metadata": {
        "id": "sdq5Wffg3g3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. Setup and Reproducibility**\n",
        "\n",
        "In this section, we import the necessary PyTorch and NumPy libraries. Crucially, we set the random seeds for all random number generators. This ensures that the initialization of weights and the shuffling of data are consistent every time we run the code, making our experiments reproducible."
      ],
      "metadata": {
        "id": "5v9EzO9q3yij"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Zq_P_owVV8OX"
      },
      "outputs": [],
      "source": [
        "# Cell 0: Library imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Reproducibility seed\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "dkb8uq3OZD0I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d06adfc-c346-453d-bb25-67c765fd5870"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. Data Acquisition**\n",
        "\n",
        "Here we download the specific \"straight vs. curly\" hair dataset from the provided URL and unzip it into the Colab environment."
      ],
      "metadata": {
        "id": "FEofQ0Tct7a8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Download and Unzip Data\n",
        "!wget -q -NL https://github.com/SVizor42/ML_Zoomcamp/releases/download/straight-curly-data/data.zip\n",
        "!unzip -q -o -d . data.zip"
      ],
      "metadata": {
        "id": "kczVJ5_1wubT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3. Model Architecture**\n",
        "\n",
        "We define the Convolutional Neural Network (CNN) class named `HairClassifier`.\n",
        "\n",
        "  * **Input:** Images of shape (3, 200, 200).\n",
        "  * **Layers:** A Convolutional layer (32 filters), Max Pooling (2x2), a Flattening layer, and two Linear (Dense) layers.\n",
        "  * **Loss Function:** We use `nn.BCEWithLogitsLoss()`. This is chosen because our final layer outputs a raw score (logit) without an activation function. This loss function combines a Sigmoid layer and the BCELoss in one single class, which is numerically more stable.\n"
      ],
      "metadata": {
        "id": "H3fTpGGYuNnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Model Architecture\n",
        "class HairClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HairClassifier, self).__init__()\n",
        "        # Input shape: (3, 200, 200)\n",
        "\n",
        "        # 1. Convolutional Layer\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # 2. Max Pooling\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        # 3. Flatten\n",
        "        # We need to calculate the flattened size.\n",
        "        # Input (200, 200) -> Conv (3x3) -> (198, 198) -> Pool (2x2) -> (99, 99)\n",
        "        # 32 filters * 99 * 99\n",
        "        self.flatten_size = 32 * 99 * 99\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # 4. Linear Layer 1\n",
        "        self.fc1 = nn.Linear(self.flatten_size, 64)\n",
        "\n",
        "        # 5. Output Layer\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize Model\n",
        "model = HairClassifier().to(device)\n",
        "\n",
        "# Q1: Loss Function\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.002, momentum=0.8)"
      ],
      "metadata": {
        "id": "RhfGEHFnuk8J"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4. Parameter Counting (Question 2)**\n",
        "\n",
        "We calculate the total number of trainable parameters in the model. This includes weights and biases for the convolutional and linear layers."
      ],
      "metadata": {
        "id": "f1RAVP8OufW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Question 2 - Count Parameters\n",
        "# We can use the manual counting method provided in the prompt\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Question 2 - Total parameters: {total_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUyj5UJmuxMQ",
        "outputId": "4c9c5e5c-9e74-4c29-944a-0f42c422afd0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 2 - Total parameters: 20073473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **5. Data Preparation (Phase 1)**\n",
        "\n",
        "We define the `train_transforms`. For the first part of the assignment, we only resize the images to 200x200, convert them to Tensors, and apply ImageNet normalization. We then create DataLoaders to feed data into the model in batches of 20.\n"
      ],
      "metadata": {
        "id": "GsSd_lzAujW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Data Preparation (Phase 1)\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((200, 200)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "# Assuming the zip created a 'data' folder with 'train' and 'test' subfolders\n",
        "train_dir = './data/train'\n",
        "test_dir = './data/test'\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transforms)\n",
        "validation_dataset = datasets.ImageFolder(root=test_dir, transform=train_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True, num_workers=2)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=20, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "gABiyY_Avgy6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **6. Training Loop (Phase 1)**\n",
        "\n",
        "This is the main training loop running for 10 epochs.\n",
        "\n",
        "  * **Training Step:** We perform the forward pass, calculate loss, perform backpropagation (`loss.backward()`), and update weights (`optimizer.step()`).\n",
        "  * **Validation Step:** We evaluate the model on unseen data without updating gradients (`torch.no_grad()`).\n",
        "  * **Metric Calculation:** Since we use `BCEWithLogitsLoss`, we manually apply `torch.sigmoid()` to the outputs to get probabilities before calculating accuracy.\n"
      ],
      "metadata": {
        "id": "yvw-NtWwuqN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Training Loop (Phase 1)\n",
        "num_epochs = 10\n",
        "history = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        labels = labels.float().unsqueeze(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    epoch_acc = correct_train / total_train\n",
        "    history['loss'].append(epoch_loss)\n",
        "    history['acc'].append(epoch_acc)\n",
        "\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in validation_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            labels = labels.float().unsqueeze(1)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_running_loss += loss.item() * images.size(0)\n",
        "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_epoch_loss = val_running_loss / len(validation_dataset)\n",
        "    val_epoch_acc = correct_val / total_val\n",
        "    history['val_loss'].append(val_epoch_loss)\n",
        "    history['val_acc'].append(val_epoch_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
        "          f\"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, \"\n",
        "          f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_cJfYmCvmgW",
        "outputId": "7c29451e-2f4c-452e-cbf8-4faa3bde8fb8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1/10, Loss: 0.6462, Acc: 0.6362, Val Loss: 0.6032, Val Acc: 0.6517\n",
            "Epoch 2/10, Loss: 0.5475, Acc: 0.7100, Val Loss: 0.7251, Val Acc: 0.6318\n",
            "Epoch 3/10, Loss: 0.5533, Acc: 0.7250, Val Loss: 0.5991, Val Acc: 0.6716\n",
            "Epoch 4/10, Loss: 0.4802, Acc: 0.7712, Val Loss: 0.6033, Val Acc: 0.6567\n",
            "Epoch 5/10, Loss: 0.4334, Acc: 0.8025, Val Loss: 0.6196, Val Acc: 0.6766\n",
            "Epoch 6/10, Loss: 0.3740, Acc: 0.8325, Val Loss: 0.7371, Val Acc: 0.6766\n",
            "Epoch 7/10, Loss: 0.2721, Acc: 0.8838, Val Loss: 0.9223, Val Acc: 0.6418\n",
            "Epoch 8/10, Loss: 0.2478, Acc: 0.9000, Val Loss: 0.7294, Val Acc: 0.7214\n",
            "Epoch 9/10, Loss: 0.2075, Acc: 0.9200, Val Loss: 0.7523, Val Acc: 0.7015\n",
            "Epoch 10/10, Loss: 0.1494, Acc: 0.9450, Val Loss: 0.7893, Val Acc: 0.7015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **7. Training Statistics (Questions 3 & 4)**\n",
        "\n",
        "We use `numpy` to calculate the median training accuracy and the standard deviation of the training loss over the 10 epochs.\n"
      ],
      "metadata": {
        "id": "YckDaUaZuw-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Answer Q3 and Q4\n",
        "import numpy as np\n",
        "\n",
        "# Question 3: Median of training accuracy\n",
        "median_train_acc = np.median(history['acc'])\n",
        "print(f\"Question 3 - Median Training Acc: {median_train_acc:.2f}\")\n",
        "\n",
        "# Question 4: Standard Deviation of training loss\n",
        "std_train_loss = np.std(history['loss'])\n",
        "print(f\"Question 4 - Std Training Loss: {std_train_loss:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgevaJY-v4OJ",
        "outputId": "6a6ddb87-9d8b-44e6-b0c9-38bbd0894b04"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 3 - Median Training Acc: 0.82\n",
            "Question 4 - Std Training Loss: 0.159\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **8. Data Augmentation Setup**\n",
        "\n",
        "We define a new transformation pipeline (`train_aug_transforms`) that includes RandomRotation, RandomResizedCrop, and RandomHorizontalFlip. We then create a new DataLoader (`train_loader_aug`). Note that the validation set does **not** use augmentation."
      ],
      "metadata": {
        "id": "SsCL_HXyu1e5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Data Augmentation Setup\n",
        "train_aug_transforms = transforms.Compose([\n",
        "    transforms.RandomRotation(50),\n",
        "    transforms.RandomResizedCrop(200, scale=(0.9, 1.0), ratio=(0.9, 1.1)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "# Reload the training dataset with the new transforms\n",
        "# Note: We do NOT re-initialize the model or optimizer\n",
        "train_dataset_aug = datasets.ImageFolder(root=train_dir, transform=train_aug_transforms)\n",
        "train_loader_aug = DataLoader(train_dataset_aug, batch_size=20, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "dE_gfdTwx3TJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **9. Continuing Training (Phase 2)**\n",
        "\n",
        "We continue training the **existing** model instance for 10 more epochs using the augmented data. We do not reset the weights; we build upon the learning from the first phase."
      ],
      "metadata": {
        "id": "RFfq17uWvQpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a new history dict for the continuation\n",
        "history_aug = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n",
        "\n",
        "print(\"Continuing training with augmentation...\")\n",
        "# Run for another 10 epochs\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for images, labels in train_loader_aug: # Use the augmented loader\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        labels = labels.float().unsqueeze(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset_aug)\n",
        "    epoch_acc = correct_train / total_train\n",
        "    history_aug['loss'].append(epoch_loss)\n",
        "    history_aug['acc'].append(epoch_acc)\n",
        "\n",
        "    # Validation (Loader remains the same as before, no augmentation on test set)\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in validation_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            labels = labels.float().unsqueeze(1)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_running_loss += loss.item() * images.size(0)\n",
        "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_epoch_loss = val_running_loss / len(validation_dataset)\n",
        "    val_epoch_acc = correct_val / total_val\n",
        "    history_aug['val_loss'].append(val_epoch_loss)\n",
        "    history_aug['val_acc'].append(val_epoch_acc)\n",
        "\n",
        "    print(f\"Aug Epoch {epoch+1}/10, \"\n",
        "          f\"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, \"\n",
        "          f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQvr_62cvRKK",
        "outputId": "a91848e7-0d7b-466b-d7c7-15799e11e024"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Continuing training with augmentation...\n",
            "Aug Epoch 1/10, Loss: 0.7249, Acc: 0.6188, Val Loss: 0.5522, Val Acc: 0.7065\n",
            "Aug Epoch 2/10, Loss: 0.6043, Acc: 0.6887, Val Loss: 0.5908, Val Acc: 0.7264\n",
            "Aug Epoch 3/10, Loss: 0.5344, Acc: 0.7288, Val Loss: 0.5284, Val Acc: 0.7264\n",
            "Aug Epoch 4/10, Loss: 0.5238, Acc: 0.7512, Val Loss: 0.5845, Val Acc: 0.7015\n",
            "Aug Epoch 5/10, Loss: 0.5096, Acc: 0.7675, Val Loss: 0.5710, Val Acc: 0.7114\n",
            "Aug Epoch 6/10, Loss: 0.4825, Acc: 0.7650, Val Loss: 0.6841, Val Acc: 0.6816\n",
            "Aug Epoch 7/10, Loss: 0.4685, Acc: 0.7650, Val Loss: 0.6649, Val Acc: 0.6667\n",
            "Aug Epoch 8/10, Loss: 0.4954, Acc: 0.7612, Val Loss: 0.5251, Val Acc: 0.7363\n",
            "Aug Epoch 9/10, Loss: 0.4763, Acc: 0.7750, Val Loss: 0.5472, Val Acc: 0.7413\n",
            "Aug Epoch 10/10, Loss: 0.4575, Acc: 0.7812, Val Loss: 0.6605, Val Acc: 0.6866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **10. Final Statistics (Questions 5 & 6)**\n",
        "\n",
        "Finally, we calculate the statistics for the second phase of training. We calculate the mean test (validation) loss and the average test accuracy over the last 5 epochs."
      ],
      "metadata": {
        "id": "FjIs0jB9u5N6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Answer Q5 and Q6\n",
        "\n",
        "# Question 5: Mean of test (validation) loss for all epochs trained with augmentations\n",
        "mean_val_loss_aug = np.mean(history_aug['val_loss'])\n",
        "print(f\"Question 5 - Mean Test Loss (Augmented): {mean_val_loss_aug:.3f}\")\n",
        "\n",
        "# Question 6: Average of test (validation) accuracy for the last 5 epochs (6 to 10)\n",
        "# Python list indexing: the last 5 items are [-5:]\n",
        "avg_val_acc_last_5 = np.mean(history_aug['val_acc'][-5:])\n",
        "print(f\"Question 6 - Avg Test Acc (Last 5 epochs): {avg_val_acc_last_5:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZcWsMySyrZp",
        "outputId": "3afef85b-7917-4e53-c231-65a525175090"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 5 - Mean Test Loss (Augmented): 0.591\n",
            "Question 6 - Avg Test Acc (Last 5 epochs): 0.70\n"
          ]
        }
      ]
    }
  ]
}